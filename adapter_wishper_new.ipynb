{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN/ViSueW5qmxWjAXLPzaKz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c96ec03a50f74870b01c654a38ce5b96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e630798698f490db5b248e6d691d4e8",
              "IPY_MODEL_ee112d27daa349548fb7f3b369a34676",
              "IPY_MODEL_cd67527c39a74ab7a7d08b6da6c4b316"
            ],
            "layout": "IPY_MODEL_5140a13653a147a6aff26f8f34587ac1"
          }
        },
        "4e630798698f490db5b248e6d691d4e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0857af087e2a400abfbb91b85eb7ac53",
            "placeholder": "​",
            "style": "IPY_MODEL_7b6a909950e844ce913051abe15143c8",
            "value": "Map: 100%"
          }
        },
        "ee112d27daa349548fb7f3b369a34676": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb75898ca18b4e2b92fc4ba9dd3fdf93",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f4e6a82bb4d4470fa57320fa8fa56601",
            "value": 3
          }
        },
        "cd67527c39a74ab7a7d08b6da6c4b316": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d23a1ec800544147a50af8b1980f94b2",
            "placeholder": "​",
            "style": "IPY_MODEL_cf095aeef1054bbda12c831e391e67c0",
            "value": " 3/3 [00:01&lt;00:00,  2.00 examples/s]"
          }
        },
        "5140a13653a147a6aff26f8f34587ac1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0857af087e2a400abfbb91b85eb7ac53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b6a909950e844ce913051abe15143c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb75898ca18b4e2b92fc4ba9dd3fdf93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4e6a82bb4d4470fa57320fa8fa56601": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d23a1ec800544147a50af8b1980f94b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf095aeef1054bbda12c831e391e67c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monoramasn/Speech_fairness/blob/main/adapter_wishper_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xFAsz4sF32V",
        "outputId": "5c46cb74-943d-4bb3-e7cd-1ff46bad2b69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.26.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.36.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3.post1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.10/dist-packages (3.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.7)\n",
            "Requirement already satisfied: rapidfuzz<4,>=3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (3.6.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install -U accelerate\n",
        "! pip install -U transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install openai-whisper"
      ],
      "metadata": {
        "id": "Dqn_dISCjwBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "import evaluate\n",
        "from dataclasses import dataclass\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from typing import Any, Dict, List, Union\n",
        "from datasets import DatasetDict, Audio, load_from_disk, concatenate_datasets\n",
        "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
        "from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer"
      ],
      "metadata": {
        "id": "SjuSgSTeG1bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "voxpopuli_dataset = load_dataset(\"facebook/voxpopuli\", \"lt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p59B5ZsIGw8_",
        "outputId": "370bd546-a00f-4ac9-a21d-8f5ab2dd68db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1429: FutureWarning: The repository for facebook/voxpopuli contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/facebook/voxpopuli\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset_lt = voxpopuli_dataset.remove_columns(['audio_id', 'language', 'raw_text', 'gender', 'speaker_id', 'is_gold_transcript', 'accent'])\n",
        "dataset_lt = voxpopuli_dataset.remove_columns(['audio_id', 'language', 'gender', 'raw_text', 'speaker_id', 'is_gold_transcript', 'accent'])"
      ],
      "metadata": {
        "id": "fh94uywoG7Ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_lt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJw9zZvAG7kh",
        "outputId": "cd08b79e-7226-4eba-80cd-d8c975bc0f5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['audio', 'normalized_text'],\n",
              "        num_rows: 456\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['audio', 'normalized_text'],\n",
              "        num_rows: 3\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['audio', 'normalized_text'],\n",
              "        num_rows: 42\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_checkpointing = True\n",
        "freeze_feature_encoder = False\n",
        "freeze_encoder = False\n",
        "\n",
        "do_normalize_eval = True\n",
        "do_lower_case = False\n",
        "do_remove_punctuation = False\n",
        "normalizer = BasicTextNormalizer()"
      ],
      "metadata": {
        "id": "gzaJdMai3ibH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint= \"openai/whisper-base\"\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_checkpoint)\n",
        "tokenizer = WhisperTokenizer.from_pretrained(model_checkpoint, language=\"Lithuanian\", task=\"transcribe\")\n",
        "processor = WhisperProcessor.from_pretrained(model_checkpoint, language=\"Lithuanian\", task=\"transcribe\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(model_checkpoint)\n",
        "\n",
        "if model.config.decoder_start_token_id is None:\n",
        "    raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n",
        "\n",
        "if freeze_feature_encoder:\n",
        "    model.freeze_feature_encoder()\n",
        "\n",
        "if freeze_encoder:\n",
        "    model.freeze_encoder()\n",
        "    model.model.encoder.gradient_checkpointing = False\n",
        "\n",
        "\n",
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []\n",
        "\n",
        "if gradient_checkpointing:\n",
        "    model.config.use_cache = False"
      ],
      "metadata": {
        "id": "DWG1V9kyG-x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint_name = model_checkpoint.split(\"/\")[-1]\n",
        "repo_name = f\"{model_checkpoint_name}-demo-colab\""
      ],
      "metadata": {
        "id": "88NkCEsUHE-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(batch):\n",
        "    # Load and resample audio data to the expected sampling rate\n",
        "    audio = batch[\"audio\"]\n",
        "    input_features = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "    #input_features = input_features.reshape(-1, 80, 3000)\n",
        "\n",
        "    # Ensure the last dimension of input_features is 3000\n",
        "    if input_features.shape[-1] < 3000:\n",
        "        padding = torch.zeros(3000 - input_features.shape[-1])\n",
        "        input_features = torch.cat([input_features, padding], dim=0)\n",
        "\n",
        "    batch[\"input_features\"] = input_features\n",
        "\n",
        "    # Compute input length of audio sample in seconds\n",
        "    batch[\"input_length\"] = len(audio[\"array\"]) / audio[\"sampling_rate\"]\n",
        "\n",
        "    #gender_mapping = {'male': 0, 'female': 1}\n",
        "    #batch[\"gender_labels\"] = gender_mapping[batch[\"gender\"].lower()]\n",
        "\n",
        "    # Optional pre-processing steps\n",
        "    transcription = batch[\"normalized_text\"]\n",
        "    if do_lower_case:\n",
        "        transcription = transcription.lower()\n",
        "    if do_remove_punctuation:\n",
        "        transcription = normalizer(transcription).strip()\n",
        "\n",
        "    # Encode target text to label ids\n",
        "    batch[\"labels\"] = processor.tokenizer(transcription, padding=\"max_length\", max_length=max_label_length).input_ids\n",
        "\n",
        "    return batch\n",
        "\n",
        "max_label_length = model.config.max_length\n",
        "min_input_length = 0.0\n",
        "max_input_length = 30.0\n",
        "def is_in_length_range(length, labels):\n",
        "    return min_input_length < length < max_input_length and 0 < len(labels) < max_label_length"
      ],
      "metadata": {
        "id": "6EdLoKcWOsaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing and ensure 'labels' key is added\n",
        "dataset_lt1 = dataset_lt.map(prepare_dataset, batch_size=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c96ec03a50f74870b01c654a38ce5b96",
            "4e630798698f490db5b248e6d691d4e8",
            "ee112d27daa349548fb7f3b369a34676",
            "cd67527c39a74ab7a7d08b6da6c4b316",
            "5140a13653a147a6aff26f8f34587ac1",
            "0857af087e2a400abfbb91b85eb7ac53",
            "7b6a909950e844ce913051abe15143c8",
            "fb75898ca18b4e2b92fc4ba9dd3fdf93",
            "f4e6a82bb4d4470fa57320fa8fa56601",
            "d23a1ec800544147a50af8b1980f94b2",
            "cf095aeef1054bbda12c831e391e67c0"
          ]
        },
        "id": "rHAEZMjC-YrF",
        "outputId": "19513194-2dc6-47bd-c518-741a61f0834e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c96ec03a50f74870b01c654a38ce5b96"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_lt1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrZ6X8_W6W1_",
        "outputId": "866a3db5-8110-45ae-c3d6-eb9611461d07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['audio', 'normalized_text', 'input_features', 'input_length', 'labels'],\n",
              "        num_rows: 456\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['audio', 'normalized_text', 'input_features', 'input_length', 'labels'],\n",
              "        num_rows: 3\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['audio', 'normalized_text', 'input_features', 'input_length', 'labels'],\n",
              "        num_rows: 42\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=dataset_lt1[\"train\"]\n",
        "val_dataset=dataset_lt1[\"test\"]"
      ],
      "metadata": {
        "id": "D4y2cgvO5mPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjZ2xRNf5xN5",
        "outputId": "88a0d6ba-876c-45f6-a84c-b0f09e7ba36d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['audio', 'normalized_text', 'input_features', 'input_length', 'labels'],\n",
              "    num_rows: 456\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        #gender_labels = [feature[\"gender_labels\"] for feature in features]\n",
        "        #batch[\"gender_labels\"] = torch.tensor(gender_labels)\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "id": "eIC5wvij3yIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
        "print('DATASET PREPARATION COMPLETED')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqpGN4utO_28",
        "outputId": "75333b05-0ef8-4e4e-db57-89bbc6b4a7ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATASET PREPARATION COMPLETED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader = DataLoader(train_dataset, batch_size=8, collate_fn=data_collator, shuffle=True)"
      ],
      "metadata": {
        "id": "nq7zt4928D-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RHowied8Ppn",
        "outputId": "0ca5d059-d459-423c-f4a4-cc80337dcd28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7fd427e04b80>"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "wer_metric = load_metric(\"wer\")\n",
        "def compute_metrics(pred):\n",
        "    pred_logits = pred.predictions\n",
        "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
        "\n",
        "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    pred_str = processor.batch_decode(pred_ids)\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
        "\n",
        "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29BXtOb232Ov",
        "outputId": "ebd61c35-9641-482c-e033-62223fd2c935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:752: FutureWarning: The repository for wer contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/wer/wer.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "adapter-whisper"
      ],
      "metadata": {
        "id": "IJia4rcPhJ-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Attention mechanism\n",
        "class WhisperAttention(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(d_model, d_model, bias=True)\n",
        "        self.q_proj = nn.Linear(d_model, d_model, bias=True)\n",
        "        self.out_proj = nn.Linear(d_model, d_model, bias=True)\n",
        "\n",
        "    def forward(self, k, v, q):\n",
        "        k = self.k_proj(k)\n",
        "        v = self.v_proj(v)\n",
        "        q = self.q_proj(q)\n",
        "\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(k.size(-1), dtype=torch.float32))\n",
        "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        context = torch.matmul(attn_probs, v)\n",
        "        return self.out_proj(context)\n",
        "\n",
        "# Encoder layer\n",
        "class WhisperEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.self_attn = WhisperAttention(d_model)\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(d_model)\n",
        "        self.activation_fn = nn.GELU()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.final_layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output = self.self_attn(x, x, x)\n",
        "        x = self.self_attn_layer_norm(x + attn_output)\n",
        "\n",
        "        fc_output = self.fc2(self.activation_fn(self.fc1(x)))\n",
        "        x = self.final_layer_norm(x + fc_output)\n",
        "        return x\n",
        "\n",
        "# Encoder\n",
        "class WhisperEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, n_layers, d_ff, max_len):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(input_dim, d_model, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv1d(d_model, d_model, kernel_size=3, stride=2, padding=1)\n",
        "        self.embed_positions = nn.Embedding(max_len, d_model)\n",
        "        self.layers = nn.ModuleList([WhisperEncoderLayer(d_model, d_ff) for _ in range(n_layers)])\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = x.transpose(1, 2)\n",
        "        position_ids = torch.arange(x.size(1), dtype=torch.long, device=x.device)\n",
        "        position_embeddings = self.embed_positions(position_ids)\n",
        "        x += position_embeddings\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = self.layer_norm(x)\n",
        "        return x\n",
        "\n",
        "# Decoder layer\n",
        "class WhisperDecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.self_attn = WhisperAttention(d_model)\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(d_model)\n",
        "        self.encoder_attn = WhisperAttention(d_model)\n",
        "        self.encoder_attn_layer_norm = nn.LayerNorm(d_model)\n",
        "        self.activation_fn = nn.GELU()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.final_layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, encoder_output):\n",
        "        self_attn_output = self.self_attn(x, x, x)\n",
        "        x = self.self_attn_layer_norm(x + self_attn_output)\n",
        "\n",
        "        enc_attn_output = self.encoder_attn(encoder_output, encoder_output, x)\n",
        "        x = self.encoder_attn_layer_norm(x + enc_attn_output)\n",
        "\n",
        "        fc_output = self.fc2(self.activation_fn(self.fc1(x)))\n",
        "        x = self.final_layer_norm(x + fc_output)\n",
        "        return x\n",
        "\n",
        "# Decoder\n",
        "class WhisperDecoder(nn.Module):\n",
        "    def __init__(self, d_model, n_layers, d_ff, max_len, vocab_size):\n",
        "        super().__init__()\n",
        "        self.embed_tokens = nn.Embedding(vocab_size, d_model, padding_idx=50257)\n",
        "        self.embed_positions = nn.Embedding(max_len, d_model)\n",
        "        self.layers = nn.ModuleList([WhisperDecoderLayer(d_model, d_ff) for _ in range(n_layers)])\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, encoder_output):\n",
        "        position_ids = torch.arange(x.size(1), dtype=torch.long, device=x.device)\n",
        "        x = self.embed_tokens(x) + self.embed_positions(position_ids)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output)\n",
        "\n",
        "        x = self.layer_norm(x)\n",
        "        return x\n",
        "\n",
        "# Adapter Layer\n",
        "class AdapterLayer(nn.Module):\n",
        "    def __init__(self, d_model, d_adapter):\n",
        "        super().__init__()\n",
        "        self.down_proj = nn.Linear(d_model, d_adapter)\n",
        "        self.activation = nn.GELU()\n",
        "        self.up_proj = nn.Linear(d_adapter, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.down_proj(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.up_proj(x)\n",
        "        return x\n",
        "\n",
        "# Complete model with Adapter Layer\n",
        "class WhisperForfinetuneWithAdapter(nn.Module):\n",
        "    def __init__(self, input_dim=80, encoder_d_model=512, encoder_n_layers=6, encoder_d_ff=2048, max_len=1500, vocab_size=51865, d_adapter=256):\n",
        "        super().__init__()\n",
        "        self.encoder = WhisperEncoder(input_dim, encoder_d_model, encoder_n_layers, encoder_d_ff, max_len)\n",
        "        self.adapter = AdapterLayer(encoder_d_model, d_adapter)\n",
        "        self.decoder = WhisperDecoder(encoder_d_model, encoder_n_layers, encoder_d_ff, max_len, vocab_size)\n",
        "        self.proj_out = nn.Linear(encoder_d_model, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, input_features, labels):\n",
        "        encoder_output = self.encoder(input_features)\n",
        "        adapter_output = self.adapter(encoder_output)\n",
        "        decoder_output = self.decoder(labels, adapter_output)\n",
        "        logits = self.proj_out(decoder_output)\n",
        "\n",
        "        outputs = {'logits': logits}\n",
        "        if labels is not None:\n",
        "          loss_fn = nn.CrossEntropyLoss()\n",
        "          loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "          outputs['loss'] = loss\n",
        "\n",
        "        return outputs\n",
        "\n",
        "# Instantiate the model with adapter\n",
        "model_with_adapter = WhisperForfinetuneWithAdapter()"
      ],
      "metadata": {
        "id": "nwlHiK4aXrxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "W6VIDtKrnfz2",
        "outputId": "1eed0913-43c9-479e-bb65-9490eb1ae251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'output' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-61353e8b2d5a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "  output_dir=repo_name,\n",
        "  group_by_length=True,\n",
        "  per_device_train_batch_size=8,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=5,\n",
        "  fp16=True,\n",
        "  gradient_checkpointing=False,\n",
        "  save_steps=50,\n",
        "  eval_steps=50,\n",
        "  logging_steps=50,\n",
        "  learning_rate=1e-4,\n",
        "  weight_decay=0.005,\n",
        "  warmup_steps=1000,\n",
        "  save_total_limit=2,\n",
        "  push_to_hub=False,\n",
        ")"
      ],
      "metadata": {
        "id": "4BszQVtk2-Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model_with_adapter,\n",
        "    train_dataset=dataset_lt1[\"train\"],\n",
        "    eval_dataset=dataset_lt1[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ],
      "metadata": {
        "id": "pZ9-dwuT3Ctf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "rnSs6gRgrUjT",
        "outputId": "101dc8ad-ef6f-49d0-f85c-c15bf8a88355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='51' max='285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 51/285 00:52 < 04:12, 0.93 it/s, Epoch 0.88/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='51' max='285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 51/285 00:54 < 04:18, 0.90 it/s, Epoch 0.88/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 27:45]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 3.46 GiB. GPU 0 has a total capacty of 15.77 GiB of which 1.41 GiB is free. Process 29479 has 14.36 GiB memory in use. Of the allocated memory 7.40 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-126-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1538\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1912\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1914\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1915\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2266\u001b[0m                     \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2267\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                 \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2269\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_seq2seq.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gen_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     def predict(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3019\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   3020\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3232\u001b[0m                     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_logits_for_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3233\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3234\u001b[0;31m                 \u001b[0mpreds_host\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreds_host\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3236\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_pad_and_concatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         return type(tensors)(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36mtorch_pad_and_concatenate\u001b[0;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtensor1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# Let's figure out the new shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.46 GiB. GPU 0 has a total capacty of 15.77 GiB of which 1.41 GiB is free. Process 29479 has 14.36 GiB memory in use. Of the allocated memory 7.40 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "QvPM6cFv3NRH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "6a8cd7e5-8357-446d-c151-8abb2203f558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:04]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.17271128296852112,\n",
              " 'eval_wer': 0.4129511677282378,\n",
              " 'eval_runtime': 43.3749,\n",
              " 'eval_samples_per_second': 1.176,\n",
              " 'eval_steps_per_second': 0.161,\n",
              " 'epoch': 5.0}"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ctMAsZOR0YdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adapter fusion with Group-dro"
      ],
      "metadata": {
        "id": "G4u23p_kIJgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Basic Adapter Layer\n",
        "class AdapterLayer(nn.Module):\n",
        "    def __init__(self, d_model, d_adapter):\n",
        "        super().__init__()\n",
        "        self.down_proj = nn.Linear(d_model, d_adapter)\n",
        "        self.activation = nn.GELU()\n",
        "        self.up_proj = nn.Linear(d_adapter, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.down_proj(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.up_proj(x)\n",
        "        return x\n",
        "\n",
        "# Gender-Specific Adapter Layer\n",
        "class GenderSpecificAdapterLayer(nn.Module):\n",
        "    def __init__(self, d_model, d_adapter):\n",
        "        super().__init__()\n",
        "        self.male_adapter = AdapterLayer(d_model, d_adapter)\n",
        "        self.female_adapter = AdapterLayer(d_model, d_adapter)\n",
        "\n",
        "    def forward(self, x, gender_labels):\n",
        "        # Apply adapters for each gender\n",
        "        male_mask = (gender_labels == 0)\n",
        "        female_mask = (gender_labels == 1)\n",
        "\n",
        "        # Apply male adapter where gender label is 0 (male)\n",
        "        x_male = self.male_adapter(x[male_mask])\n",
        "\n",
        "        # Apply female adapter where gender label is 1 (female)\n",
        "        x_female = self.female_adapter(x[female_mask])\n",
        "\n",
        "        # Ensure that both parts are of the same data type\n",
        "        if x.dtype != x_male.dtype:\n",
        "            x_male = x_male.to(x.dtype)\n",
        "        if x.dtype != x_female.dtype:\n",
        "            x_female = x_female.to(x.dtype)\n",
        "\n",
        "        # Reassemble the output tensor, preserving original order\n",
        "        output = torch.zeros_like(x)\n",
        "        output[male_mask] = x_male\n",
        "        output[female_mask] = x_female\n",
        "\n",
        "        return output\n",
        "\n",
        "class AdapterFusionLayer(nn.Module):\n",
        "    def __init__(self, d_model, d_adapter):\n",
        "        super().__init__()\n",
        "        self.gender_adapters = GenderSpecificAdapterLayer(d_model, d_adapter)\n",
        "\n",
        "    def forward(self, x, gender_labels):\n",
        "        return self.gender_adapters(x, gender_labels)\n",
        "\n",
        "# Attention Mechanism\n",
        "class WhisperAttention(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(d_model, d_model, bias=True)\n",
        "        self.q_proj = nn.Linear(d_model, d_model, bias=True)\n",
        "        self.out_proj = nn.Linear(d_model, d_model, bias=True)\n",
        "\n",
        "    def forward(self, k, v, q):\n",
        "        k = self.k_proj(k)\n",
        "        v = self.v_proj(v)\n",
        "        q = self.q_proj(q)\n",
        "\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(k.size(-1), dtype=torch.float32))\n",
        "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        context = torch.matmul(attn_probs, v)\n",
        "        return self.out_proj(context)\n",
        "\n",
        "# Encoder Layer with Gender Adapter\n",
        "class WhisperEncoderLayerWithGenderAdapter(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, d_adapter):\n",
        "        super().__init__()\n",
        "        self.self_attn = WhisperAttention(d_model)\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(d_model)\n",
        "        self.activation_fn = nn.GELU()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.final_layer_norm = nn.LayerNorm(d_model)\n",
        "        self.gender_adapter = GenderSpecificAdapterLayer(d_model, d_adapter)\n",
        "\n",
        "    def forward(self, x, gender_labels):\n",
        "        attn_output = self.self_attn(x, x, x)\n",
        "        x = self.self_attn_layer_norm(x + attn_output)\n",
        "\n",
        "        fc_output = self.fc2(self.activation_fn(self.fc1(x)))\n",
        "        x = self.final_layer_norm(x + fc_output)\n",
        "\n",
        "        x = self.gender_adapter(x, gender_labels)\n",
        "        return x\n",
        "\n",
        "# Encoder with Gender Adapter\n",
        "class WhisperEncoderWithGenderAdapter(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, n_layers, d_ff, max_len, d_adapter):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(input_dim, d_model, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv1d(d_model, d_model, kernel_size=3, stride=2, padding=1)\n",
        "        self.embed_positions = nn.Embedding(max_len, d_model)\n",
        "        self.layers = nn.ModuleList([WhisperEncoderLayerWithGenderAdapter(d_model, d_ff, d_adapter) for _ in range(n_layers)])\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, gender_label):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = x.transpose(1, 2)\n",
        "        position_ids = torch.arange(x.size(1), dtype=torch.long, device=x.device)\n",
        "        position_embeddings = self.embed_positions(position_ids)\n",
        "        x += position_embeddings\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, gender_label)\n",
        "\n",
        "        x = self.layer_norm(x)\n",
        "        return x\n",
        "\n",
        "# Decoder Layer with Gender Adapter\n",
        "class WhisperDecoderLayerWithGenderAdapter(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, d_adapter):\n",
        "        super().__init__()\n",
        "        self.self_attn = WhisperAttention(d_model)\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(d_model)\n",
        "        self.encoder_attn = WhisperAttention(d_model)\n",
        "        self.encoder_attn_layer_norm = nn.LayerNorm(d_model)\n",
        "        self.activation_fn = nn.GELU()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.final_layer_norm = nn.LayerNorm(d_model)\n",
        "        self.gender_adapter = GenderSpecificAdapterLayer(d_model, d_adapter)\n",
        "\n",
        "    def forward(self, x, encoder_output, gender_labels):\n",
        "        self_attn_output = self.self_attn(x, x, x)\n",
        "        x = self.self_attn_layer_norm(x + self_attn_output)\n",
        "\n",
        "        enc_attn_output = self.encoder_attn(encoder_output, encoder_output, x)\n",
        "        x = self.encoder_attn_layer_norm(x + enc_attn_output)\n",
        "\n",
        "        fc_output = self.fc2(self.activation_fn(self.fc1(x)))\n",
        "        x = self.final_layer_norm(x + fc_output)\n",
        "\n",
        "        x = self.gender_adapter(x, gender_labels)\n",
        "        return x\n",
        "\n",
        "# Decoder with Gender Adapter\n",
        "class WhisperDecoderWithGenderAdapter(nn.Module):\n",
        "    def __init__(self, d_model, n_layers, d_ff, max_len, vocab_size, d_adapter):\n",
        "        super().__init__()\n",
        "        self.embed_tokens = nn.Embedding(vocab_size, d_model, padding_idx=50257)\n",
        "        self.embed_positions = nn.Embedding(max_len, d_model)\n",
        "        self.layers = nn.ModuleList([WhisperDecoderLayerWithGenderAdapter(d_model, d_ff, d_adapter) for _ in range(n_layers)])\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, encoder_output, gender_labels):\n",
        "        position_ids = torch.arange(x.size(1), dtype=torch.long, device=x.device)\n",
        "        x = self.embed_tokens(x) + self.embed_positions(position_ids)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, gender_labels)\n",
        "\n",
        "        x = self.layer_norm(x)\n",
        "        return x\n",
        "\n",
        "# Complete Model with Gender-Specific Adapter Layers\n",
        "\n",
        "class WhisperfinetuneWithAdapterFusion(nn.Module):\n",
        "    def __init__(self, input_dim=80, encoder_d_model=512, encoder_n_layers=6, encoder_d_ff=2048, max_len=1500, vocab_size=51865, d_adapter=256):\n",
        "        super().__init__()\n",
        "        # Pass d_adapter to the encoder\n",
        "        self.encoder = WhisperEncoderWithGenderAdapter(input_dim, encoder_d_model, encoder_n_layers, encoder_d_ff, max_len, d_adapter)\n",
        "        self.adapter_fusion = AdapterFusionLayer(encoder_d_model, d_adapter)\n",
        "        self.decoder = WhisperDecoderWithGenderAdapter(encoder_d_model, encoder_n_layers, encoder_d_ff, max_len, vocab_size, d_adapter)\n",
        "        self.proj_out = nn.Linear(encoder_d_model, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, input_features, labels, gender_labels):\n",
        "        # Pass gender_labels to the encoder\n",
        "        encoder_output = self.encoder(input_features, gender_labels)\n",
        "        fused_adapter_output = self.adapter_fusion(encoder_output, gender_labels)\n",
        "        decoder_output = self.decoder(labels, fused_adapter_output, gender_labels)\n",
        "        logits = self.proj_out(decoder_output)\n",
        "\n",
        "        outputs = {'logits': logits}\n",
        "        if labels is not None:\n",
        "          loss_fn = nn.CrossEntropyLoss()\n",
        "          loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "          outputs['loss'] = loss\n",
        "\n",
        "        return outputs\n",
        "\n",
        "AdapterFusion = WhisperfinetuneWithAdapterFusion(input_dim=80, encoder_d_model=512, encoder_n_layers=6, encoder_d_ff=2048, max_len=1500, vocab_size=51865, d_adapter=256)"
      ],
      "metadata": {
        "id": "-HCkDKnEHJ35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XFUUHjcez54c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Assuming these dimensions based on your model definition\n",
        "batch_size = 1\n",
        "audio_feature_size = 80  # Number of features in your audio data\n",
        "seq_length = 3000        # Sequence length of your audio data\n",
        "\n",
        "# Create dummy audio features\n",
        "dummy_audio_features = torch.randn(batch_size, audio_feature_size, seq_length)\n",
        "\n",
        "# Create dummy labels (e.g., for sequence-to-sequence, these could be token IDs)\n",
        "vocab_size = 51865       # Vocabulary size (based on your model)\n",
        "dummy_labels = torch.randint(low=0, high=vocab_size, size=(batch_size, 100)) # Example label sequence\n",
        "\n",
        "# Create dummy gender labels (0 for male, 1 for female for example)\n",
        "dummy_gender_labels = torch.tensor([0]) # Assuming batch size of 1\n",
        "\n",
        "output = AdapterFusion(dummy_audio_features, dummy_labels, dummy_gender_labels)"
      ],
      "metadata": {
        "id": "mUpaCTMhHKcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_audio_features.shape\n",
        "dummy_labels.shape\n",
        "#dummy_gender_labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edWAqV6513fY",
        "outputId": "4ae59652-ec33-4e8b-98d8-8997a978dd7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer\n",
        "from torch.optim import Adam\n",
        "learning_rate = 0.001\n",
        "optimizer = Adam(AdapterFusion.parameters(), lr=learning_rate)\n",
        "\n",
        "def train_one_epoch(AdapterFusion, data_loader, optimizer, device):\n",
        "    AdapterFusion.train()\n",
        "    for batch in data_loader:\n",
        "        input_features = batch['input_features'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        gender_labels = batch['gender_labels'].to(device)\n",
        "\n",
        "        outputs = AdapterFusion(input_features, labels, gender_labels)\n",
        "        logits = outputs['logits']\n",
        "\n",
        "        # Compute loss for each gender group\n",
        "        loss_male = F.cross_entropy(logits[gender_labels == 0], labels[gender_labels == 0]) if (gender_labels == 0).any() else torch.tensor(0.)\n",
        "        loss_female = F.cross_entropy(logits[gender_labels == 1], labels[gender_labels == 1]) if (gender_labels == 1).any() else torch.tensor(0.)\n",
        "\n",
        "        # Choose the highest loss for backpropagation\n",
        "        worst_group_loss = max(loss_male, loss_female)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        worst_group_loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "NuQZvWukz6Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rFddrVIK3lhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from transformers import Seq2SeqTrainer, TrainingArguments\n",
        "\n",
        "class GenderAwareTrainer(Seq2SeqTrainer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.scaler = GradScaler()\n",
        "\n",
        "    def training_step(self, model, inputs):\n",
        "        gender_labels = inputs.pop(\"gender_labels\")\n",
        "        use_amp = self.args.fp16\n",
        "\n",
        "        with autocast(enabled=use_amp):\n",
        "            outputs = model(**inputs, gender_labels=gender_labels)\n",
        "            loss = outputs['loss']\n",
        "\n",
        "        self.scaler.scale(loss).backward()\n",
        "        self.scaler.step(self.optimizer)\n",
        "        self.optimizer.zero_grad(set_to_none=True)\n",
        "        self.scaler.update()\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "f0SUj6jQFYwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "  output_dir=repo_name,\n",
        "  group_by_length=True,\n",
        "  per_device_train_batch_size=8,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=5,\n",
        "  fp16=True,\n",
        "  gradient_checkpointing=False,\n",
        "  save_steps=50,\n",
        "  eval_steps=50,\n",
        "  logging_steps=50,\n",
        "  learning_rate=1e-4,\n",
        "  weight_decay=0.005,\n",
        "  warmup_steps=1000,\n",
        "  save_total_limit=2,\n",
        "  push_to_hub=False,\n",
        ")"
      ],
      "metadata": {
        "id": "9k31uLLHFppg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the custom trainer\n",
        "trainer = GenderAwareTrainer(\n",
        "    args=training_args,\n",
        "    model=AdapterFusion,  # Make sure this is your gender-aware model\n",
        "    train_dataset=dataset_lt1[\"train\"],\n",
        "    eval_dataset=dataset_lt1[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "5Qq4mgGmF3C3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IlFKBKKZdNmw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}